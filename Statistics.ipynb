{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FMm0axcmig7E",
        "X2UpTrvzbUWf",
        "qkxHFE6TdiDO",
        "YALJsETee1sL",
        "HqvXjFnGi3cM"
      ]
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhmvmApIe0Ke"
      },
      "source": [
        "# Statistics\n",
        "\n",
        "In this notebook, you will work with CO2 emissions data and life expectancy data from the [Gapminder Foundation](https://www.gapminder.org/). You will learn some basics of data exploration using R, such as inferring the underlying data distributions and testing statistical hypotheses on this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMTseKvlb_kH"
      },
      "source": [
        "# Load packages\n",
        "library(ggplot2)   # Plotting package"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QXoLIjIENND"
      },
      "source": [
        "# Set plot size for upcoming plots\n",
        "options(repr.plot.width = 12, repr.plot.height = 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu1V6daUhFU9"
      },
      "source": [
        "First, we begin by reading the CSV files directly from the Github directory (the original data can be obtained from https://www.gapminder.org/data/). Because the CSV files were saved using Swiss format, we must specify the column separator to be a semi-colon, the default separator is a comma and this is what you will encounter in most CSV files. In general, when loading a CSV file, if the data in R looks very strange, you should make sure the separator matches that of the CSV file (open it with a text editor, e.g. Notepad, to see what the separator is)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbwYvigffMAw"
      },
      "source": [
        "# Read CSVs from the Github directory directly.\n",
        "co2_emissions <- read.csv(\"https://raw.githubusercontent.com/SEPS-UniSG/statistics/main/data/co2_emissions_tonnes_per_person.csv\", sep = \";\") #co2 per capita by country\n",
        "co2_emissions_total <- read.csv(\"https://raw.githubusercontent.com/SEPS-UniSG/statistics/main/data/consumption_co2_emissions_1000_tonnes.csv\") # co2 total by country\n",
        "life_expectancy <- read.csv(\"https://raw.githubusercontent.com/SEPS-UniSG/statistics/main/data/life_expectancy_years.csv\", sep = \";\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMm0axcmig7E"
      },
      "source": [
        "## CO2 Emissions\n",
        "We start by working with the CO2 emissions data. Let us begin by displaying the first few lines of the dataset to better understand how the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgwuGroffMDC"
      },
      "source": [
        "head(co2_emissions, 10) # Displays the first 10 lines of the co2_emissions dataset\n",
        "print(paste0(\"Number of countries:\", nrow(co2_emissions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NAs in 2010 and 2018? (years that we focus on)\n",
        "any(is.na(co2_emissions$X2010))\n",
        "any(is.na(co2_emissions$X2018))"
      ],
      "metadata": {
        "id": "cRAyJf35XRVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aBvAI5Ji3Qc"
      },
      "source": [
        "Taking the data from 2018 (column `X2018`), we plot the histogram and mean of the cross-sectional data and display its summary statistics.\n",
        "\n",
        "### A note on plotting in R\n",
        "R offers a few simple plotting functions in its base packages. There is, however, a library called `ggplot2` that provides an amazing toolbox for custom plots. This is the most widely used plotting library in R.\n",
        "\n",
        "The basic R plots have the advantage of being easier to understand and they provide the minimum needed in a few lines. `ggplot2` on the other hand, is a bit more complicated to understand but it brings plotting to another level, this is especially useful when writing a paper or assignment.\n",
        "\n",
        "In this notebook, we will provide some basic R plot codes as well as some `ggplot2` plot codes as each have their own advantages and disadvantages. When you want to quickly look at a histogram or scatterplot of your data, the basic plotting function will do the job just fine. But when you want to create a fancy plot for an assignment or presentation, you will be glad to know how to do it with `ggplot2`!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2caOcB9jPh9"
      },
      "source": [
        "mu <- mean(co2_emissions$X2018) # Store the mean\n",
        "# Compute a vector of breaks for the histogram (this is generally not needed),\n",
        "# however, ggplot2 and the base histogram use different methods to compute and\n",
        "# display histograms, hence this is needed to ensure that the 2 plots match\n",
        "breaks <- seq(0, 40, by = 1)\n",
        "# Plot the histogram (breaks refers to the number of bins)\n",
        "hist(co2_emissions$X2018, xlab = \"Tonnes per person\",\n",
        "  main = \"CO2 Emissions (base R plot)\", breaks = breaks)\n",
        "# Plot the mean as a dashed vertical red line\n",
        "abline(v = mu, col = \"red\", lty = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecCmeixWKnIq"
      },
      "source": [
        "The above code for plotting was quite straightforward, as we see below, `ggplot2` is slightly more complex. When first implementing your own plots using `ggplot2`, it's always a good idea to have a look at https://ggplot2.tidyverse.org/index.html where you will find many examples of different plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98jyIopiCgW0"
      },
      "source": [
        "# ggplot \"stacks\" multiple layers the first function defines the underlying data\n",
        "# and the aesthetics, i.e. the x- and y-axis but also the color/fill based on data\n",
        "ggplot(data = co2_emissions, aes(x = X2018)) +\n",
        "  # Add a histogram layer with a binwidth of 1, black borders and lightblue fill\n",
        "  geom_histogram(binwidth = 1, color = \"black\", fill = \"lightblue\", boundary = 0) +\n",
        "  # Add labels to the x- and y-axis as well as a title\n",
        "  labs(x = \"Tonnes per person\", y = \"Frequency\", title = \"CO2 Emissions (ggplot2)\") +\n",
        "  # Add a vertical line at x = mu with a blue color and a dashed linetype\n",
        "  geom_vline(xintercept = mu, color = \"blue\", lty = \"dashed\") +\n",
        "  # Add a \"theme\" to your plot, try changing it to one of the following:\n",
        "  # theme_gray, theme_dark, theme_void, theme_minimal\n",
        "  theme_bw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMhZi1mKIhCC"
      },
      "source": [
        "# Displays the summary statistics of co2_emissions$X2018\n",
        "summary(co2_emissions$X2018)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yPPERpklxQ5"
      },
      "source": [
        "The histogram hints toward the data following a **Poisson** or an **exponential** distribution. Both the Poisson and exponential distributions are fully defined by their rate parameter $\\lambda$, which is also equal to the mean for the Poisson distribution and equal to the inverse of the mean for the exponential distribution.\n",
        "\n",
        "To get a clearer visual intuition of how the distributions line up with the data, we overlay the probability density functions with rate parameter $\\hat{\\lambda}$ estimated through maximum-likelihood estimation.\n",
        "\n",
        "Furthermore, we plot the empirical cumulative distribution function of the data and compare it to a plot the cumulative distribution functions of the Poisson and exponential distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eusb77ilvU_V"
      },
      "source": [
        "### A note on probability distributions in R\n",
        "As a statistical programming language, R offers many useful statistical functions in its base libraries. For instance, a lot of probability distributions come directly with 4 functions, e.g. consider a standard normal random variable $X \\sim \\mathcal{N}(0, 1)$:\n",
        "+ `dnorm(x)` returns the probability density function evaluated at `x`, i.e. $f(x) = \\frac{e^{-\\frac{x^2}{2}}}{\\sqrt{2\\pi}}$.\n",
        "+ `pnorm(x)` returns the cumulative distribution function evaluated at `x`, i.e. $F(x) = P[-\\infty  \\leq X < x]$.\n",
        "+ `qnorm(p)` returns the quantile function evaluated at `p`, i.e. $Q(p) = \\inf\\{x \\in \\mathbb{R} : p \\leq F(x)\\}$.\n",
        "+ `rnorm(n)` returns a vector of length $n$ where each entry of the vector is randomly distributed according to a standard normal distribution.\n",
        "\n",
        "These 4 functions also exist for other types of probability distribution, e.g. `dpois`, `ppois`, `qpois`, `rpois` for a Poisson distribution, `dunif`, `punif`, `qunif`, `runif` for a uniform distribution, etc. These functions will be extremely handy in a lot of cases so be sure to play around and understand how they work.\n",
        "\n",
        "To have a look at the parameter required by the functions, you can either use `?function_name` or `help(function_name)`, e.g. `?dpois` will give information on the probability density function for the Poisson distribution in R."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoGTcSbwjPkF"
      },
      "source": [
        "xrange <- 0:40 # Define a range for the x-axis\n",
        "# Plot the histogram\n",
        "hist(co2_emissions$X2018, xlab = \"Tonnes per person\", main = \"CO2 Emissions\", breaks = 30, freq = FALSE)\n",
        "# Overlay the probability density functions\n",
        "lines(xrange, dpois(xrange, mu), lty = 2, col = \"red\")\n",
        "lines(xrange, dexp(xrange, 1/mu), lty = 2, col = \"blue\")\n",
        "# Add a legend\n",
        "legend(x = 25, y = 0.25, legend = c(\"Poisson PDF\", \"Exponential PDF\"),\n",
        "  fill = c(\"red\", \"blue\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiVFyzXljPmW"
      },
      "source": [
        "# Plot the empirical CDF of the data\n",
        "plot(ecdf(co2_emissions$X2018), main = \"Empirical Cumulative Distribution Function\",\n",
        "  ylab = \"P(X < x)\")\n",
        "# Plot the true CDFs\n",
        "lines(xrange, ppois(xrange, mu), lty = 2, col = \"red\")\n",
        "lines(xrange, pexp(xrange, 1/mu), lty = 2, col = \"blue\")\n",
        "legend(x = 25, y = 0.9, legend = c(\"Empirical CDF\", \"Poisson CDF\", \"Exponential PDF\"),\n",
        "  fill = c(\"black\", \"red\", \"blue\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_bTdMQbQ1Qq"
      },
      "source": [
        "From this visual test, we conclude that the exponential distribution with rate parameter $\\lambda = \\frac{1}{\\bar{x}}$ is a better fit for the life CO2 emissions data sample than the Poisson distribution with rate parameter $\\lambda = \\bar{x}$ (where $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i \\approx 4.46$ is the sample mean)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdIUzxeIr5OC"
      },
      "source": [
        "### Evolution of average CO2 emissions over the years\n",
        "We now turn to the question of whether the average CO2 emissions have evolved over the years. To answer this question, we compare the mean of the data from 2010 to that of 2018."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gidGJV2r8D5"
      },
      "source": [
        "# Compute the mean CO2 emissions for 2010\n",
        "mu_2010 <- mean(co2_emissions$X2010, na.rm = TRUE)\n",
        "mu_2010"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmchZEcfsQZ2"
      },
      "source": [
        "#### Confidence Interval\n",
        "Before we go on with the hypothesis testing, we build a 95% confidence interval around the mean from the 2010 data ($\\frac{1}{N}\\sum_{i=1}^N x_i = \\bar{X}_{2010} = 4.68$).\n",
        "\n",
        "Recall that, by the [central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem), that for a sequence of i.i.d. variables $\\{X_1, X_2, \\dots, X_n\\}$, for large enough $n$, the sampling distribution of the mean approaches normality, i.e. $\\bar{X}_n \\sim \\mathcal{N}(\\mu, \\frac{\\sigma^2}{n})$ where $\\mu = \\mathbb{E}[X_i]$ and $\\sigma^2 = \\mathbb{V}[X_i]$ are the variables' mean and variance respectively.\n",
        "\n",
        "Hence we can compute the 95% confidence interval for the 2010 data by using the 2.5% and 97.5% quantiles of a normal distribution with mean $\\hat{\\mu}_\\mu = \\hat{\\mu} = \\bar{X}_{2010}$ and standard deviation $\\hat{\\sigma}_\\mu = \\frac{\\hat{\\sigma}}{\\sqrt{N}} = \\sqrt{\\frac{\\sum_{i=1}^N (x_i - \\hat{\\mu})^2}{N(N-1)}}$, where the subscript $_\\mu$ is used to make it explicit that these values refer to the sampling distribution of the mean and not to the data itself.\n",
        "\n",
        "The confidence interval can then be written as $CI = \\hat{\\mu}_\\mu \\pm z_{(1-\\alpha/2)} \\hat{\\sigma}_\\mu$, where $z_{(1-\\alpha/2)}$ is the $100(1-\\frac{\\alpha}{2})^\\text{th}$ quantile of a standard normal distribution. This is $\\approx 1.64$ for a 10% confidence level ($\\alpha = 0.1$) and $\\approx 1.96$ for a 5% confidence level ($\\alpha = 0.05$). Oftentimes you will just encounter these numbers instead of the exact quantiles, so it is important to know what they stand for."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kSgduHIsM0E"
      },
      "source": [
        "# Compute the confidence interval bounds for the 2010 mean\n",
        "N <- sum(!is.na(co2_emissions$X2010))\n",
        "lb <- mu_2010 - qnorm(.975) * sd(co2_emissions$X2010, na.rm = TRUE) / sqrt(N)\n",
        "ub <- mu_2010 + qnorm(.975) * sd(co2_emissions$X2010, na.rm = TRUE) / sqrt(N)\n",
        "print(paste0(\"Confidence interval: [\", round(lb, 2), \"; \", round(ub, 2), \"]\"))\n",
        "print(paste0(\"Sample mean for 2018: \",\n",
        "  round(mean(co2_emissions$X2018, na.rm = TRUE), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3lEN0zJ-J7r"
      },
      "source": [
        "We see that our confidence interval is $CI = [3.82; 5.55]$ and hence the sample mean we obtained for the 2018 data ($4.46$) is well within the bounds of the confidence interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG12bf-fkNpU"
      },
      "source": [
        "#### Hypothesis Test\n",
        "Given our visual inspection above, we assume that CO2 emissions follow an exponential distribution. As we have estimated the rate parameter for the 2010 data, we denote $\\lambda_0 = \\frac{1}{4.68}$ the parameter of our target hypothesis. Our hypothesis is:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H_0&: \\lambda = \\lambda_0 \\\\\n",
        "H_1&: \\lambda \\neq \\lambda_0,\n",
        "\\end{aligned}\n",
        "$$\n",
        "i.e. we test the target hypothesis that the rate parameter of the data in 2018 is the same as that in 2010 against the alternative hypothesis that the rate parameter (and hence the mean) differs. We set the level of our test to be $\\alpha = 0.05$ and thus we have a 5% probability to reject $H_0$ if it were in fact true (type I error).\n",
        "\n",
        "We proceed now with a slightly more complicated approach: we use a likelihood-ratio test as described in **section 9.11** of Dudewicz and Mishra (1988, p. 514). Let $\\Theta_0 = \\{\\lambda_0\\}$ be the set of parameters under the target hypothesis and $\\Theta = \\{\\lambda : \\lambda > 0\\}$ the set of all possible parameters. The likelihood-ratio test statistic is then defined as:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "LR &= \\frac{\\underset{\\lambda \\in \\Theta_0}{\\max}f(x|\\lambda)}{\\underset{\\lambda \\in \\Theta}{\\max}f(x|\\lambda)} \\\\\n",
        "&= \\frac{\\lambda_0^N e^{-\\lambda_0 \\sum_{i=1}^N x_i}}{\\hat{\\lambda}^N e^{-\\hat{\\lambda} \\sum_{i=1}^N x_i}}, \\ \\text{with} \\ \\hat{\\lambda} = \\frac{1}{\\bar{x}}=\\left(\\frac{1}{N}\\sum_{i=1}^N x_i\\right)^{-1} = \\frac{1}{4.46} \\\\\n",
        "&= \\left(\\frac{\\lambda_0}{\\hat{\\lambda}}\\right)^N e^{\\left(\\hat{\\lambda}-\\lambda_0\\right)\\sum_{i=1}^N x_i}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRXM_hCwbTj4"
      },
      "source": [
        "Since $\\Theta_0 \\subset \\Theta$, one of two cases can occur:\n",
        "1. $\\hat{\\lambda} = \\lambda_0$, the maximum likelihood estimate of $\\lambda$ is equal to $\\lambda_0$ and thus $LR = 1$\n",
        "2. $\\hat{\\lambda} \\neq \\lambda_0 \\Rightarrow f(x|\\hat{\\lambda}) > f(x|\\lambda_0) \\Rightarrow LR < 1$\n",
        "\n",
        "In consequence, $LR$ will be between $0$ and $1$: the higher the $LR$, the more evidence it provides for the target hypothesis, and, conversely, a low $LR$ provides evidence against the target hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rF_jMEAdzRK"
      },
      "source": [
        "Hence, we want to reject the target hypothesis when $LR$ is small enough, i.e. when $\\mathbb{P}_{\\lambda_0}[LR < k] = \\alpha$ for a given level $\\alpha$.\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\alpha = 0.05 &= \\mathbb{P}_{\\lambda_0}[LR < k] \\\\\n",
        "&= \\mathbb{P}_{\\lambda_0}[\\log(LR) < \\log(k)] \\\\\n",
        "&= \\mathbb{P}_{\\lambda_0}[N\\log \\left(\\frac{\\lambda_0}{\\hat{\\lambda}}\\right) + (\\hat{\\lambda}-\\lambda_0) \\sum_{i=1}^N x_i < \\log(k)] \\\\\n",
        "&= \\mathbb{P}_{\\lambda_0}\\left[\\sum_{i=1}^N x_i < \\underbrace{\\frac{\\log(k) - N\\log \\left(\\frac{\\lambda_0}{\\hat{\\lambda}}\\right)}{(\\hat{\\lambda}-\\lambda_0)}}_{=k^\\prime}\\right] \\\\\n",
        "&= \\mathbb{P}_{\\lambda_0}\\left[\\sum_{i=1}^N x_i < k^\\prime \\right]\n",
        "\\end{aligned}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjA_Bux1j0V8"
      },
      "source": [
        "Now, since under $H_0$, it holds that $\\sum_{i=1}^N x_i \\sim \\Gamma \\left(x \\bigg|n-1, \\frac{1}{\\lambda_0}, 0\\right)$ and we want a level of $\\alpha = 0.05$, we must set $k^\\prime = q_{0.05}$, where $q_{0.05}$ is the 5th percentile of the Gamma distribution under $H_0$. This gives us a 5% chance of type I error, that is: rejecting $H_0$ if it indeed were true."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbmZgZ2XqDBy"
      },
      "source": [
        "# Compute lambda_0 from the 2010 data\n",
        "lambda_0 <- 1/mean(co2_emissions$X2010, na.rm = TRUE)\n",
        "# Compute N from the 2018 data\n",
        "N <- sum(!is.na(co2_emissions$X2018))\n",
        "# Compute k prime under the target hypothesis (notice that the shape is not N-1\n",
        "# but N and the rate is not 1/lambda_0 but lambda_0, this is due to a different\n",
        "# specification of the Gamma distribution in R than in Dudewicz & Mishra,\n",
        "# see later on in this notebook for a brief discussion)\n",
        "k <- qgamma(0.05, shape = N, rate = lambda_0)\n",
        "# Compute the sum of observations in 2018\n",
        "xs <- sum(co2_emissions$X2018, na.rm = TRUE)\n",
        "# Print the results\n",
        "print(paste0(\"k prime = \", round(k, digits = 2)))\n",
        "print(paste0(\"Sum of x = \", round(xs, digits = 2)))\n",
        "# Reject H0 if the sum of x's is smaller than k prime\n",
        "print(ifelse(xs < k, \"Reject H0\", \"Fail to reject H0\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvVXe7VdXJAF"
      },
      "source": [
        "As the computations above show, we find $k^\\prime = 795.46$ and $\\sum_{i=1}^N x_i = 855.37$. Consequently, $\\sum_{i=1}^N x_i > k^\\prime$, hence we fail to reject $H_0$ at the 95% significance level.\n",
        "\n",
        "*Given our hypothesis test and the data at hand, we cannot reject that the CO2 emissions measured in 2018 are the same as those measured in 2010.*\n",
        "\n",
        "Remark: We could also use alternative approaches to analyse this question, e.g. testing for difference in means in 2010 and 2018 using a t-test."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print means of outcome variable: did co2 per capita increase?\n",
        "paste0(\"Mean CO2 2010: \",round(mu_2010,2))\n",
        "paste0(\"Mean CO2 per capitper capita a 2018: \",round(mu,2))\n"
      ],
      "metadata": {
        "id": "ce1DdMDapsJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Are you surprised about the result?**\n",
        "\n",
        "We would think that CO2-emissions increase over time (in means), but here they decreased in means and we cannot reject that CO2 emissions stay the same.\n",
        "\n",
        "Maybe... the data is the problem?\n",
        "We used CO2-emissions per capita, with every country having the same weight no matter what population.\n",
        "Maybe it is more interesting to look at total CO2-emissions by country? Lets do that!"
      ],
      "metadata": {
        "id": "T66s-BBRY29K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total CO2-emissions\n",
        "Lets repeat the same investigation as before, just for for total CO2-emissions by country instead of CO2-emission per capita.\n",
        "\n",
        "In detail, data data includes the total carbon dioxide emissions from fossile fuel consumption, cememt production, gas flaring, minus export, plus import during the given year, by country."
      ],
      "metadata": {
        "id": "fCYBE4jJZuI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading and preparation"
      ],
      "metadata": {
        "id": "hmZucEmrbZ7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head(co2_emissions_total, 10) # Displays the first 10 lines of the co2_emissions dataset\n",
        "paste(\"Number of countries:\" ,nrow(co2_emissions_total))"
      ],
      "metadata": {
        "id": "zNTYrU65bL6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "any(is.na(co2_emissions_total))"
      ],
      "metadata": {
        "id": "aCZfsgQwbN1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to match right character\n",
        "lastCharacter <- function(x, n=1){\n",
        "  substr(x, nchar(x)-n+1, nchar(x))\n",
        "}\n",
        "\n",
        "# function to transform strings to numbers\n",
        "string_to_number <- function(input_string) {\n",
        "  multiplier <- 1\n",
        "  if (lastCharacter(input_string)==\"k\") {\n",
        "    multiplier <- 1000\n",
        "    input_string <- gsub(\"k\", \"\", input_string)\n",
        "  } else if (lastCharacter(input_string)==\"M\") {\n",
        "    multiplier <- 1e6\n",
        "    input_string <- gsub(\"M\", \"\", input_string)\n",
        "  }\n",
        "  return(as.numeric(input_string) * multiplier)\n",
        "}\n",
        "\n",
        "# transform data to numeric:\n",
        "co2_emissions_total <- as.data.frame(cbind(co2_emissions_total[,1], apply(co2_emissions_total[,-1], c(1, 2), string_to_number) )) #transform strings\n",
        "co2_emissions_total[, 2:ncol(co2_emissions_total)] <- lapply(co2_emissions_total[, 2:ncol(co2_emissions_total)], as.numeric) # set cols to numeric (apart from country)\n",
        "colnames(co2_emissions_total)[1] <- \"country\"\n",
        "\n",
        "\n",
        "# have a look!\n",
        "head(co2_emissions_total)"
      ],
      "metadata": {
        "id": "LlLSNOJBbPu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu <- mean(co2_emissions_total$X2017) # Store the mean for 2017\n",
        "mu_2010 <- mean(co2_emissions_total$X2010) # Store the mean for 2010\n",
        "print(paste0(\"Mean in 2010: \",mu_2010))\n",
        "print(paste0(\"Mean in 2017: \",mu))"
      ],
      "metadata": {
        "id": "tiwvIweycqJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histogram and exponential distribution\n",
        "Note that the data goes only until 2017, so we will use 2017 instead of 2018 for our analysis."
      ],
      "metadata": {
        "id": "X2UpTrvzbUWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ggplot \"stacks\" multiple layers the first function defines the underlying data\n",
        "# and the aesthetics, i.e. the x- and y-axis but also the color/fill based on data\n",
        "ggplot(data = co2_emissions_total, aes(x = X2017)) +\n",
        "  # Add a histogram layer with a binwidth of 1, black borders and lightblue fill\n",
        "  geom_histogram(binwidth = 100000, color = \"black\", fill = \"lightblue\", boundary = 0) +\n",
        "  # Add labels to the x- and y-axis as well as a title\n",
        "  labs(x = \"1000 Tonnes\", y = \"Frequency\", title = \"CO2 Emissions (ggplot2)\") +\n",
        "  # Add a vertical line at x = mu with a blue color and a dashed linetype\n",
        "  geom_vline(xintercept = mu, color = \"blue\", lty = \"dashed\") +\n",
        "  # Add a \"theme\" to your plot, try changing it to one of the following:\n",
        "  # theme_gray, theme_dark, theme_void, theme_minimal\n",
        "  theme_bw()"
      ],
      "metadata": {
        "id": "PC9AWlo6btCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xrange <- 0:10000000 # Define a range for the x-axis\n",
        "# Plot the histogram\n",
        "hist(co2_emissions_total$X2017, xlab = \"Tonnes per person\", main = \"CO2 Emissions\", breaks = 50, freq = FALSE)\n",
        "# Overlay the probability density functions\n",
        "lines(xrange, dpois(xrange, mu), lty = 2, col = \"red\")\n",
        "lines(xrange, dexp(xrange, 1/mu), lty = 2, col = \"blue\")\n",
        "# Add a legend\n",
        "legend(x = 6000000, y = 0.00013, legend = c(\"Poisson PDF\", \"Exponential PDF\"),\n",
        "  fill = c(\"red\", \"blue\"))"
      ],
      "metadata": {
        "id": "bSfEtBAhcSyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the empirical CDF of the data\n",
        "plot(ecdf(co2_emissions_total$X2017), main = \"Empirical Cumulative Distribution Function\",\n",
        "  ylab = \"P(X < x)\")\n",
        "# Plot the true CDFs\n",
        "lines(xrange, ppois(xrange, mu), lty = 2, col = \"red\")\n",
        "lines(xrange, pexp(xrange, 1/mu), lty = 2, col = \"blue\")\n",
        "legend(x = 25, y = 0.9, legend = c(\"Empirical CDF\", \"Poisson CDF\", \"Exponential PDF\"),\n",
        "  fill = c(\"black\", \"red\", \"blue\"))"
      ],
      "metadata": {
        "id": "SUJJERG7cZUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this visual test, we conclude that the exponential distribution with rate parameter $\\lambda = \\frac{1}{\\bar{x}}$ is a better fit for the life CO2 emissions data sample than the Poisson distribution with rate parameter $\\lambda = \\bar{x}$ (where $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i \\approx 282400.8$ is the sample mean)."
      ],
      "metadata": {
        "id": "aYVc1ovldLD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence interval\n",
        "Lets build the 95% confidence interval around the co2-emissions from 2010 again, using the CLT."
      ],
      "metadata": {
        "id": "qkxHFE6TdiDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the confidence interval bounds for the 2010 mean\n",
        "N <- sum(!is.na(co2_emissions_total$X2010))\n",
        "lb <- mu_2010 - qnorm(.975) * sd(co2_emissions_total$X2010, na.rm = TRUE) / sqrt(N)\n",
        "ub <- mu_2010 + qnorm(.975) * sd(co2_emissions_total$X2010, na.rm = TRUE) / sqrt(N)\n",
        "print(paste0(\"Confidence interval: [\", round(lb, 2), \"; \", round(ub, 2), \"]\"))\n",
        "print(paste0(\"Sample mean for 2018: \",\n",
        "  round(mean(co2_emissions_total$X2017, na.rm = TRUE), 2)))"
      ],
      "metadata": {
        "id": "iJK-78DgdwAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis test\n",
        "Again, we conduct the likelihood-ratio test to test:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H_0&: \\lambda = \\lambda_0 \\\\\n",
        "H_1&: \\lambda \\neq \\lambda_0,\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "YALJsETee1sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute lambda_0 from the 2010 data\n",
        "lambda_0 <- 1/mean(co2_emissions_total$X2010, na.rm = TRUE)\n",
        "# Compute N from the 2018 data\n",
        "N <- sum(!is.na(co2_emissions_total$X2017))\n",
        "# Compute k prime under the target hypothesis (notice that the shape is not N-1\n",
        "# but N and the rate is not 1/lambda_0 but lambda_0, this is due to a different\n",
        "# specification of the Gamma distribution in R than in Dudewicz & Mishra,\n",
        "# see later on in this notebook for a brief discussion)\n",
        "k <- qgamma(0.05, shape = N, rate = lambda_0)\n",
        "# Compute the sum of observations in 2018\n",
        "xs <- sum(co2_emissions_total$X2017, na.rm = TRUE)\n",
        "# Print the results\n",
        "print(paste0(\"k prime = \", round(k, digits = 2)))\n",
        "print(paste0(\"Sum of x = \", round(xs, digits = 2)))\n",
        "# Reject H0 if the sum of x's is smaller than k prime\n",
        "print(ifelse(xs < k, \"Reject H0\", \"Fail to reject H0\"))"
      ],
      "metadata": {
        "id": "3CVGNqTyfKLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the computations above show, we find $k^\\prime = 726,632,202.53$ and $\\sum_{i=1}^N x_i = 33,605,700$. Consequently, $\\sum_{i=1}^N x_i > k^\\prime$, hence we fail to reject $H_0$ at the 95% significance level.\n",
        "\n",
        "*Again, given our hypothesis test and the data at hand, we cannot reject that the total CO2 emissions measured in 2017 are the same as those measured in 2010.*\n",
        "\n",
        "\n",
        "Remark: Country-level variation is very high in this dataset, which might lead to no statistical significance. Doing the same exercise with:\n",
        "- a different time horizon\n",
        "- a subset of \"large\" countries\n",
        "- a data transformation (e.g. logs)\n",
        "might lead to very different results."
      ],
      "metadata": {
        "id": "f39oAu-5fWnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Are you surprised about the result?**\n",
        "\n",
        "We would think that CO2-emissions increase over time (in means), but here they decreased in means and we cannot reject that CO2 emissions stay the same.\n",
        "\n",
        "Maybe... the data is the problem?\n",
        "We used CO2-emissions per capita, with every country having the same weight no matter what population.\n",
        "Maybe it is more interesting to look at total CO2-emissions by country? Lets do that!"
      ],
      "metadata": {
        "id": "T66s-BBRY29K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Total CO2-emissions\n",
        "Lets repeat the same investigation as before, just for for total CO2-emissions by country instead of CO2-emission per capita.\n",
        "\n",
        "In detail, data data includes the total carbon dioxide emissions from fossile fuel consumption, cememt production, gas flaring, minus export, plus import during the given year, by country."
      ],
      "metadata": {
        "id": "fCYBE4jJZuI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data loading and preparation"
      ],
      "metadata": {
        "id": "hmZucEmrbZ7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "head(co2_emissions_total, 10) # Displays the first 10 lines of the co2_emissions dataset\n",
        "paste(\"Number of countries:\" ,nrow(co2_emissions_total))"
      ],
      "metadata": {
        "id": "zNTYrU65bL6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "any(is.na(co2_emissions_total))"
      ],
      "metadata": {
        "id": "aCZfsgQwbN1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to match right character\n",
        "lastCharacter <- function(x, n=1){\n",
        "  substr(x, nchar(x)-n+1, nchar(x))\n",
        "}\n",
        "\n",
        "# function to transform strings to numbers\n",
        "string_to_number <- function(input_string) {\n",
        "  multiplier <- 1\n",
        "  if (lastCharacter(input_string)==\"k\") {\n",
        "    multiplier <- 1000\n",
        "    input_string <- gsub(\"k\", \"\", input_string)\n",
        "  } else if (lastCharacter(input_string)==\"M\") {\n",
        "    multiplier <- 1e6\n",
        "    input_string <- gsub(\"M\", \"\", input_string)\n",
        "  }\n",
        "  return(as.numeric(input_string) * multiplier)\n",
        "}\n",
        "\n",
        "# transform data to numeric:\n",
        "co2_emissions_total <- as.data.frame(cbind(co2_emissions_total[,1], apply(co2_emissions_total[,-1], c(1, 2), string_to_number) )) #transform strings\n",
        "co2_emissions_total[, 2:ncol(co2_emissions_total)] <- lapply(co2_emissions_total[, 2:ncol(co2_emissions_total)], as.numeric) # set cols to numeric (apart from country)\n",
        "colnames(co2_emissions_total)[1] <- \"country\"\n",
        "\n",
        "\n",
        "# have a look!\n",
        "head(co2_emissions_total)"
      ],
      "metadata": {
        "id": "LlLSNOJBbPu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mu <- mean(co2_emissions_total$X2017) # Store the mean for 2017\n",
        "mu_2010 <- mean(co2_emissions_total$X2010) # Store the mean for 2010\n",
        "print(paste0(\"Mean in 2010: \",mu_2010))\n",
        "print(paste0(\"Mean in 2017: \",mu))"
      ],
      "metadata": {
        "id": "tiwvIweycqJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histogram and exponential distribution\n",
        "Note that the data goes only until 2017, so we will use 2017 instead of 2018 for our analysis."
      ],
      "metadata": {
        "id": "X2UpTrvzbUWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ggplot \"stacks\" multiple layers the first function defines the underlying data\n",
        "# and the aesthetics, i.e. the x- and y-axis but also the color/fill based on data\n",
        "ggplot(data = co2_emissions_total, aes(x = X2017)) +\n",
        "  # Add a histogram layer with a binwidth of 1, black borders and lightblue fill\n",
        "  geom_histogram(binwidth = 100000, color = \"black\", fill = \"lightblue\", boundary = 0) +\n",
        "  # Add labels to the x- and y-axis as well as a title\n",
        "  labs(x = \"1000 Tonnes\", y = \"Frequency\", title = \"CO2 Emissions (ggplot2)\") +\n",
        "  # Add a vertical line at x = mu with a blue color and a dashed linetype\n",
        "  geom_vline(xintercept = mu, color = \"blue\", lty = \"dashed\") +\n",
        "  # Add a \"theme\" to your plot, try changing it to one of the following:\n",
        "  # theme_gray, theme_dark, theme_void, theme_minimal\n",
        "  theme_bw()"
      ],
      "metadata": {
        "id": "PC9AWlo6btCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xrange <- 0:10000000 # Define a range for the x-axis\n",
        "# Plot the histogram\n",
        "hist(co2_emissions_total$X2017, xlab = \"Tonnes per person\", main = \"CO2 Emissions\", breaks = 50, freq = FALSE)\n",
        "# Overlay the probability density functions\n",
        "lines(xrange, dpois(xrange, mu), lty = 2, col = \"red\")\n",
        "lines(xrange, dexp(xrange, 1/mu), lty = 2, col = \"blue\")\n",
        "# Add a legend\n",
        "legend(x = 6000000, y = 0.00013, legend = c(\"Poisson PDF\", \"Exponential PDF\"),\n",
        "  fill = c(\"red\", \"blue\"))"
      ],
      "metadata": {
        "id": "bSfEtBAhcSyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the empirical CDF of the data\n",
        "plot(ecdf(co2_emissions_total$X2017), main = \"Empirical Cumulative Distribution Function\",\n",
        "  ylab = \"P(X < x)\")\n",
        "# Plot the true CDFs\n",
        "lines(xrange, ppois(xrange, mu), lty = 2, col = \"red\")\n",
        "lines(xrange, pexp(xrange, 1/mu), lty = 2, col = \"blue\")\n",
        "legend(x = 25, y = 0.9, legend = c(\"Empirical CDF\", \"Poisson CDF\", \"Exponential PDF\"),\n",
        "  fill = c(\"black\", \"red\", \"blue\"))"
      ],
      "metadata": {
        "id": "SUJJERG7cZUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this visual test, we conclude that the exponential distribution with rate parameter $\\lambda = \\frac{1}{\\bar{x}}$ is a better fit for the life CO2 emissions data sample than the Poisson distribution with rate parameter $\\lambda = \\bar{x}$ (where $\\bar{x} = \\frac{1}{N}\\sum_{i=1}^N x_i \\approx 282400.8$ is the sample mean)."
      ],
      "metadata": {
        "id": "aYVc1ovldLD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence interval\n",
        "Lets build the 95% confidence interval around the co2-emissions from 2010 again, using the CLT."
      ],
      "metadata": {
        "id": "qkxHFE6TdiDO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the confidence interval bounds for the 2010 mean\n",
        "N <- sum(!is.na(co2_emissions_total$X2010))\n",
        "lb <- mu_2010 - qnorm(.975) * sd(co2_emissions_total$X2010, na.rm = TRUE) / sqrt(N)\n",
        "ub <- mu_2010 + qnorm(.975) * sd(co2_emissions_total$X2010, na.rm = TRUE) / sqrt(N)\n",
        "print(paste0(\"Confidence interval: [\", round(lb, 2), \"; \", round(ub, 2), \"]\"))\n",
        "print(paste0(\"Sample mean for 2018: \",\n",
        "  round(mean(co2_emissions_total$X2017, na.rm = TRUE), 2)))"
      ],
      "metadata": {
        "id": "iJK-78DgdwAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothesis test\n",
        "Again, we conduct the likelihood-ratio test to test:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H_0&: \\lambda = \\lambda_0 \\\\\n",
        "H_1&: \\lambda \\neq \\lambda_0,\n",
        "\\end{aligned}\n",
        "$$"
      ],
      "metadata": {
        "id": "YALJsETee1sL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute lambda_0 from the 2010 data\n",
        "lambda_0 <- 1/mean(co2_emissions_total$X2010, na.rm = TRUE)\n",
        "# Compute N from the 2018 data\n",
        "N <- sum(!is.na(co2_emissions_total$X2017))\n",
        "# Compute k prime under the target hypothesis (notice that the shape is not N-1\n",
        "# but N and the rate is not 1/lambda_0 but lambda_0, this is due to a different\n",
        "# specification of the Gamma distribution in R than in Dudewicz & Mishra,\n",
        "# see later on in this notebook for a brief discussion)\n",
        "k <- qgamma(0.05, shape = N, rate = lambda_0)\n",
        "# Compute the sum of observations in 2018\n",
        "xs <- sum(co2_emissions_total$X2017, na.rm = TRUE)\n",
        "# Print the results\n",
        "print(paste0(\"k prime = \", round(k, digits = 2)))\n",
        "print(paste0(\"Sum of x = \", round(xs, digits = 2)))\n",
        "# Reject H0 if the sum of x's is smaller than k prime\n",
        "print(ifelse(xs < k, \"Reject H0\", \"Fail to reject H0\"))"
      ],
      "metadata": {
        "id": "3CVGNqTyfKLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the computations above show, we find $k^\\prime = 726,632,202.53$ and $\\sum_{i=1}^N x_i = 33,605,700$. Consequently, $\\sum_{i=1}^N x_i > k^\\prime$, hence we fail to reject $H_0$ at the 95% significance level.\n",
        "\n",
        "*Again, given our hypothesis test and the data at hand, we cannot reject that the total CO2 emissions measured in 2017 are the same as those measured in 2010.*\n",
        "\n",
        "\n",
        "Remark: Country-level variation is very high in this dataset, which might lead to no statistical significance. Doing the same exercise with:\n",
        "- a different time horizon\n",
        "- a subset of \"large\" countries\n",
        "- a data transformation (e.g. logs)\n",
        "might lead to very different results."
      ],
      "metadata": {
        "id": "f39oAu-5fWnJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqvXjFnGi3cM"
      },
      "source": [
        "## Life Expectancy\n",
        "We now turn to the life expectancy dataset, similarly to what we did above, we begin by showing the histogram of the data and compare it to the PDFs of well-known distributions.\n",
        "\n",
        "We begin by showing the first 10 rows of the dataframe, just to see what our data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_akUxdofMFo"
      },
      "source": [
        "head(life_expectancy, 10) # Displays the first 10 lines of the life_expectancy dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS7Mo9YcIZGc"
      },
      "source": [
        "The first thing we notice is that the life expectancy data goes up to the year 2100. This is unexpected and prompts question on the underlying data. In general, if you encounter something similar in your own research / data exploration, it is crucial to check why the data looks the way it does.\n",
        "\n",
        "In this case, we find [a summary documentation of the data](https://www.gapminder.org/data/documentation/gd004/) on the original website. As it turns out, data from 2017 to 2099 is computed using UN forecasts from *World Population Prospects*, while the data from 1970 to 2016 originates from the Institue of Health Metrics and Evaluation. Consequently, one should be particularly careful when comparing the data through the years, indeed if we were to repeat the ideas above and compare the 2018 and 2010 data, we would be comparing predictions to past measurements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkHA-_GXa_6e"
      },
      "source": [
        "This short digression aside, we return to our data and plot the histogram of the 2016 life expectancy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwrqz27WfMHc"
      },
      "source": [
        "# Store the mean, because we have NA data, we must use na.rm = TRUE to get a valid mean\n",
        "mu <- mean(life_expectancy$X2016, na.rm = TRUE)\n",
        "# Plot the histogram\n",
        "hist(life_expectancy$X2016, xlab = \"Years\", main = \"Average life expectancy\", breaks = 20)\n",
        "# Plot the mean as a dashed vertical red line\n",
        "abline(v = mu, col = \"red\", lty = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nReugBHsSz5C"
      },
      "source": [
        "Considering the above histogram we decide to compare it to two different distributions: a **displaced exponential** distribution, and a **Gamma** distribution. To do so, we will do as in the first part of this notebook and overlay their probability distribution functions on top of the histogram. Furthermore, we will then compare their cumulative distribution function to the empirical CDF of the data.\n",
        "\n",
        "The displaced exponential is not part of the base probability functions in R. However, the distribution without displacement exists, we will use this to build our custom PDF and CDF which allow for a displacement parameter.\n",
        "\n",
        "In practice, you can also obtain the displaced exponential distribution functions from the `tolerance` library. This is slightly easier and quicker than building your own function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL70ysnIW0s9"
      },
      "source": [
        "### Displaced Exponential\n",
        "In Dudewicz and Mishra (1988), the PDF of the displaced exponential is given by **Definition 4.2.10**:\n",
        "$$ f_X(x) =\n",
        "\\begin{cases}\n",
        "  \\frac{1}{\\beta} e^{-\\frac{(x-A)}{\\beta}}, &A \\leq x < \\infty \\\\\n",
        "  0, &\\text{otherwise.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Note that this definition is slightly different from what you will find in most modern software or in the [Wikipedia article](https://en.wikipedia.org/wiki/Exponential_distribution). These notation use the parameter $\\lambda$, the *rate*. This is simply the inverse of $\\beta$, i.e. $\\lambda = \\frac{1}{\\beta}$. For instance, R uses $\\lambda$ instead of $\\beta$, just make sure to check which parameters are used in your software of choice to avoid mistakes.\n",
        "\n",
        "As mentioned above, R does not provide a displaced exponential in its base libraries, only a *standard* exponential. It is easy, however, to write a displaced exponential PDF in the form of a *standard* exponential PDF.\n",
        "\n",
        "Let $X$ be a displaced exponential RV with PDF $f_X$ as described above and let $Y$ be a non-displaced exponential RV with PDF $f_Y$:\n",
        "$$ f_Y(y) =\n",
        "\\begin{cases}\n",
        "  \\frac{1}{\\beta_Y} e^{-\\frac{y}{\\beta_Y}}, &0 \\leq y < \\infty \\\\\n",
        "  0, &\\text{otherwise.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Assume you want to evalute the PDF $f_X(x)$ of a displaced exponential with displacement parameter $A$, it suffices to evaluate the pdf of the non-displaced exponential (with the rate also displaced by $A$, i.e. setting $\\frac{1}{\\beta_Y} = \\frac{1}{\\beta}-A$) at $x-A$, i.e. $f_X(x) = f_Y(x-A)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZOTC0vCWyG6"
      },
      "source": [
        "# Writing our own functions for the displaced exponential CDF and PDF\n",
        "# Simply use the base non-displaced function on a displaced x value. Notice also\n",
        "# that the rate parameter used in R is equal to 1/beta\n",
        "ddexp <- function(x, beta, A) { dexp(x-A, rate = 1/beta) }\n",
        "pdexp <- function(x, beta, A) { pexp(x-A, rate = 1/beta) }\n",
        "# Compute the displacement parameter as the minimum observation in the data\n",
        "A <- min(life_expectancy$X2016, na.rm = TRUE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mla3LybxbnfU"
      },
      "source": [
        "### Gamma\n",
        "The Gamma distribution is defined by a shape parameter $\\alpha$, a rate parameter $\\beta$, and a displacement parameter $A$ and its PDF is given by:\n",
        "$$\n",
        "f_X(x)=\n",
        "\\begin{cases}\n",
        "  \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} (x-A)^{\\alpha-1}e^{-\\beta (x-A)},& \\text{if } x \\geq A\\\\\n",
        "    0,              & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "You might notice that this PDF is not the same as the one you will find in **Definition 4.2.7** of Dudewicz and Mishra (1988). Once again, we take the CDF and PDF from [Wikipedia](https://en.wikipedia.org/wiki/Gamma_distribution) as it is generally closer to what you will find in modern software.\n",
        "\n",
        "Of course, it is easy to show that both PDFs are equivalent, denote the parameters in Dudewicz and Mishra by a subscript of $\\text{DM}$, we can simply replace $\\beta$ and $\\alpha$ with $\\beta = \\frac{1}{\\beta_\\text{DM}}$ and $\\alpha = \\alpha_\\text{DM} + 1$ respectively, which gives the PDF as shown in **Definition 4.2.7** (Dudewicz and Mishra, 1988) or in the course slides:\n",
        "$$\n",
        "f_X(x)=\n",
        "\\begin{cases}\n",
        "  \\frac{1}{\\beta_{\\text{DM}}^{\\alpha_\\text{DM}+1}\\Gamma(\\alpha_\\text{DM}+1)} (x-A)^{\\alpha_\\text{DM}}e^{-\\frac{(x-A)}{\\beta_\\text{DM}}},& \\text{if } x \\geq A\\\\\n",
        "    0,              & \\text{otherwise}\n",
        "\\end{cases}$$\n",
        "\n",
        "We estimate both shape and rate parameters by method of moments, i.e. $\\alpha = \\frac{\\mathbb{E}[X]^2}{\\mathbb{V}[X]}$ and $\\beta = \\frac{\\mathbb{E}[X]}{\\mathbb{V}[X]}$, where $\\mathbb{V}$ symbolizes the variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx-uI-P0sUXb"
      },
      "source": [
        "xrange <- seq(50, 90, by = 0.1)\n",
        "# Estimate the shape and rate parameters by method of moments\n",
        "alpha <- mu^2 / var(life_expectancy$X2016, na.rm = TRUE)\n",
        "beta <- alpha / mu\n",
        "# Plot the histogram\n",
        "hist(life_expectancy$X2016, xlab = \"Years\", main = \"Average life expectancy\",\n",
        "  breaks = 20, freq = FALSE)\n",
        "# Overlay the probability density function of a displaced exponential distribution\n",
        "lines(xrange, ddexp(xrange, mean(life_expectancy$X2016 - A, na.rm = TRUE), A), lty = 2, col = \"red\")\n",
        "# Overlay the probability density function of a Gamma distribution\n",
        "lines(xrange, dgamma(xrange, shape = alpha, rate = beta), lty = 2, col = \"blue\")\n",
        "# Add a legend\n",
        "legend(x = 51, y = 0.06, legend = c(\"Displaced exponential PDF\", \"Gamma PDF\"), fill = c(\"red\", \"blue\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mzAzIOFfwvT"
      },
      "source": [
        "From the above histogram with overlayed PDFs, it is quite clear that the Gamma distribution matches the data much better than the displaced exponential. Nevertheless, we also plot the empirical CDF of the data compared with both distributions' CDFs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaYStGGPUGOL"
      },
      "source": [
        "# Plot the empirical CDF of the data\n",
        "plot(ecdf(life_expectancy$X2016), main = \"Empirical Cumulative Distribution Functions\",\n",
        "  ylab = \"P(X < x)\")\n",
        "# Plot the true CDF of the displaced exponential\n",
        "lines(xrange, pdexp(xrange, mu - A, A), lty = 2, col = \"red\")\n",
        "# Plot the true CDF of the Gamma distribution\n",
        "lines(xrange, pgamma(xrange, shape = alpha, rate = beta), lty = 2, col = \"blue\")\n",
        "# Add a legend\n",
        "legend(x = 50, y = 0.95, legend = c(\"Empirical CDF\", \"Displaced exponential CDF\",\n",
        "  \"Gamma CDF\"), fill = c(\"black\", \"red\", \"blue\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRd-cVBPi96Z"
      },
      "source": [
        "This plot gives a visual confirmation that our choice of a Gamma distribution fits the data much better than the displaced exponential.\n",
        "\n",
        "Finally, we would like to see if the average life expectancy has changed over the past years but this time we take a different approach. We apply bootstrapping in order to find 95% confidence intervals around the average life expectancy average in 2010. We then compare the average life expectancy from 1975 to 2016 and see if it is within the 95% confidence interval of 2010.\n",
        "\n",
        "Note that we take a look at two different bootstrapping procedures:\n",
        "1. We draw samples from the observed data directly, this is a purely empirical approach.\n",
        "2. We draw samples from a Gamma distribution with shape $\\hat{\\alpha}$ and rate $\\hat{\\beta}$, where both parameters are estimated by method of moments:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "\\hat{\\alpha} &= \\frac{\\mathbb{E}[X]^2}{\\mathbb{V}[X]} = \\frac{\\frac{1}{N}\\left(\\sum_{i=1}^N x_i\\right)^2}{\\frac{1}{N}\\left[\\sum_{i=1}^N \\left( x_i - \\frac{1}{N}\\left(\\sum_{i=1}^N x_i\\right)\\right)^2\\right]} \\\\\n",
        "\\hat{\\beta} &= \\frac{\\mathbb{E}[X]}{\\mathbb{V}[X]} = \\frac{\\hat{\\alpha}}{\\frac{1}{N}\\left(\\sum_{i=1}^N x_i\\right)}\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcps0_IY4P_q"
      },
      "source": [
        "# Compute population means and variance (R only computes sample variance)\n",
        "N <- sum(!is.na(life_expectancy$X2010))\n",
        "mu <- mean(life_expectancy$X2010, na.rm = TRUE)\n",
        "sigma2 <- var(life_expectancy$X2010, na.rm = TRUE) * (N - 1) / N\n",
        "# Estimate shape and rate by method of moments\n",
        "alpha <- mu^2 / sigma2\n",
        "beta <- alpha / mu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bootstrap of the mean and hypothesis test\n",
        "The general idea of a bootstrap is to draw many data samples and compute the parameter of interest for each sample. The resulting distribution of the parameter of interest is used e.g. to compute standard erorrs, do hypothesis testing or construct confidence intervals.\n",
        "\n",
        "\n",
        "**Sampling techniques:**\n",
        "- Empirical bootstraps sample from the data at hand WITH replacement, using the same sample size.\n",
        "- Distributional bootstraps sample from a distribution (which should fit the data), e.g. here the gamma distribution with estimated parameters."
      ],
      "metadata": {
        "id": "4UHQdX4oq9ix"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTjOM67xgDfd"
      },
      "source": [
        "# Bootstrap mean values\n",
        "n_rep <- 2000         # Number of Bootstrap repetitions\n",
        "# Initialize a vector of NAs to store the means for both approaches\n",
        "mu_empirical <- rep(NA, n_rep)\n",
        "mu_distributional <- rep(NA, n_rep)\n",
        "for (i in 1:n_rep) {  # Iterate over the repetitions\n",
        "  # Sample from the observed data with replacement, same size as the original data\n",
        "  bsamp <- sample(life_expectancy$X2010, length(life_expectancy$X2010),\n",
        "    replace = TRUE)\n",
        "  # Sample from the Gamma distribution with fitted parameters\n",
        "  bdist <- rgamma(n_rep, shape = alpha, rate = beta)\n",
        "  # Estimate mean of the bootstrap sample and store it in the vector of means\n",
        "  mu_empirical[i] <- mean(bsamp)\n",
        "  mu_distributional[i] <- mean(bdist)\n",
        "}\n",
        "# Extract the 2.5% and 97.5% quantiles of the bootstrapped means\n",
        "lb_empirical <- quantile(mu_empirical, 0.025, names = FALSE)\n",
        "ub_empirical <- quantile(mu_empirical, 0.975, names = FALSE)\n",
        "lb_distributional <- quantile(mu_distributional, 0.025, names = FALSE)\n",
        "ub_distributional <- quantile(mu_distributional, 0.975, names = FALSE)\n",
        "# Print a comparison of both confidence intervals\n",
        "print(paste0(\"'Empirical' bootstrap CI of mean:      [\",\n",
        "  round(lb_empirical, digits = 2), \"; \", round(ub_empirical, digits = 2), \"]\"))\n",
        "print(paste0(\"'Distributional' bootstrap CI of mean: [\",\n",
        "  round(lb_distributional, digits = 2), \"; \", round(ub_distributional, digits = 2), \"]\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1D2j9jeGg-V"
      },
      "source": [
        "# Compare the means obtained by the different bootstrap procedures using histograms\n",
        "hist(mu_empirical, xlim = c(68, 73))\n",
        "hist(mu_distributional, xlim = c(68, 73))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KvH59AzHCUm"
      },
      "source": [
        "The histograms above clearly shows how the means obtained by the *distributional* bootstrapping procedure are much more concentrated than the ones obtained by the *empirical* approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZnyF3vE5C2W"
      },
      "source": [
        "# In this cell we create a dataframe to help us plot the data with ggplot\n",
        "# Create a sequence of years\n",
        "years <- 1975:2016\n",
        "# Extract means by column for each of the above years\n",
        "means <- colMeans(life_expectancy[, paste0(\"X\", years)], na.rm = TRUE)\n",
        "# Create the dataframe for plotting\n",
        "df_plot <- data.frame(year = years, mean = means)\n",
        "# Add two columns to denote whether the mean value is below, above, or within\n",
        "# the bootstrapped 'empirical' and 'distributional' confidence intervals\n",
        "df_plot$CI_emp_diff <- sapply(df_plot$mean, function (x) {\n",
        "  if (x < lb_empirical) {\n",
        "    \"Below CI\"\n",
        "  } else if (x <= ub_empirical) {\n",
        "    \"Within CI\"\n",
        "  } else {\n",
        "    \"Above CI\"\n",
        "  }\n",
        "})\n",
        "# \"Distributional\" bootstrap CI\n",
        "df_plot$CI_dist_diff <- sapply(df_plot$mean, function (x) {\n",
        "  if (x < lb_distributional) {\n",
        "    \"Below CI\"\n",
        "  } else if (x <= ub_distributional) {\n",
        "    \"Within CI\"\n",
        "  } else {\n",
        "    \"Above CI\"\n",
        "  }\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC2OnPFWF0CU"
      },
      "source": [
        "# Create a plot using ggplot with the underlying data being \"df_plot\",\n",
        "# the x-axis is defined by the \"year\" column and the y-axis by the \"mean\" column,\n",
        "# the fill of each bar is also defined by the \"mean\" column\n",
        "\n",
        "# Plot for the empirical bootstrap\n",
        "plot_empirical <- ggplot(data = df_plot, aes(x = year, y = mean, fill = CI_emp_diff)) +\n",
        "  # Display the data as bars (columns)\n",
        "  geom_col() +\n",
        "  # Add custom fill colors (see: http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf)\n",
        "  # for color names in R, or use Hex codes (see: https://www.color-hex.com/)\n",
        "  scale_fill_manual(values = c(\"Above CI\" = \"springgreen4\",\n",
        "    \"Within CI\" = \"#4a88ff\", \"Below CI\" = \"tomato\")) +\n",
        "  # Scale the x-axis in a custom manner\n",
        "  scale_x_continuous(breaks = seq(1975, 2016, by = 5)) +\n",
        "  # Add confidence intervals (based on empirical bootstrap)\n",
        "  geom_hline(yintercept = c(lb_empirical, ub_empirical), linetype = \"dotted\") +\n",
        "  # Add labels to x- and y-axis, the fill legend as well as a title\n",
        "  labs(x = \"Year\", y = \"Age\", fill = \"\",\n",
        "  title = \"Life Expectancy with bootstrapped 95% confidence intervals (based on 2010, 'empirical' bootstrap)\") +\n",
        "  # Prettify the plot using a theme\n",
        "  theme_bw()\n",
        "\n",
        "# Plot for the distributional bootstrap\n",
        "plot_distributional <- ggplot(data = df_plot, aes(x = year, y = mean, fill = CI_dist_diff)) +\n",
        "  geom_col() +\n",
        "  scale_fill_manual(values = c(\"Above CI\" = \"springgreen4\",\n",
        "    \"Within CI\" = \"#4a88ff\", \"Below CI\" = \"tomato\")) +\n",
        "  scale_x_continuous(breaks = seq(1975, 2016, by = 5)) +\n",
        "  # Add confidence intervals (based on distributional bootstrap)\n",
        "  geom_hline(yintercept = c(lb_distributional, ub_distributional), linetype = \"dotted\") +\n",
        "  labs(x = \"Year\", y = \"Age\", fill = \"\",\n",
        "  title = \"Life Expectancy with bootstrapped 95% confidence intervals (based on 2010, 'distributional' bootstrap)\") +\n",
        "  theme_bw()\n",
        "\n",
        "# Display both plots\n",
        "plot_empirical\n",
        "plot_distributional"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc2OW_s28nXw"
      },
      "source": [
        "Based on the above plot, we see that the average life expectancy has significantly increased over the past years. There is a clear trend of life expectancy increasing over time, and we notice how the average life expectancy is significantly below that of 2010 in the preceding years (pre-2006 according to the *empirical* bootstrap and pre-2009 according to the *distributional* one), while it is significantly above in the following years (post-2013 according to the *empirical* bootstrap and post-2010 according to the *distributional* one)."
      ]
    }
  ]
}